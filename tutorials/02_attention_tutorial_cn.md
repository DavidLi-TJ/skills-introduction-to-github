# æ•™ç¨‹ 2: æ³¨æ„åŠ›æœºåˆ¶ä»é›¶å¼€å§‹

## ğŸ¯ å­¦ä¹ ç›®æ ‡

å­¦å®Œæœ¬æ•™ç¨‹åï¼Œä½ å°†èƒ½å¤Ÿï¼š
1. ç†è§£æ³¨æ„åŠ›æœºåˆ¶çš„ç›´è§‰
2. æŒæ¡ Queryã€Keyã€Value çš„æ¦‚å¿µ
3. å®ç°ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›
4. ç†è§£å¤šå¤´æ³¨æ„åŠ›çš„ä¼˜åŠ¿

## ğŸ¤” ä¸ºä»€ä¹ˆéœ€è¦æ³¨æ„åŠ›ï¼Ÿ

### ç”Ÿæ´»ä¸­çš„ä¾‹å­

æƒ³è±¡ä½ åœ¨è¯»ä¸€ç¯‡æ–‡ç« ï¼š

> "å°æ˜å»å•†åº—ä¹°äº†è‹¹æœï¼Œç„¶å**ä»–**å›å®¶äº†ã€‚"

å½“ä½ è¯»åˆ°"ä»–"è¿™ä¸ªå­—æ—¶ï¼Œä½ çš„å¤§è„‘ä¼šè‡ªåŠ¨"æ³¨æ„"åˆ°å‰é¢çš„"å°æ˜"ï¼ŒçŸ¥é“"ä»–"æŒ‡çš„æ˜¯è°ã€‚è¿™å°±æ˜¯**æ³¨æ„åŠ›**ï¼

### æœºå™¨ç¿»è¯‘çš„æŒ‘æˆ˜

ç¿»è¯‘ "I love you" â†’ "æˆ‘çˆ±ä½ "

- ç¿»è¯‘"æˆ‘"æ—¶ï¼Œåº”è¯¥ä¸»è¦çœ‹"I"
- ç¿»è¯‘"çˆ±"æ—¶ï¼Œåº”è¯¥ä¸»è¦çœ‹"love"  
- ç¿»è¯‘"ä½ "æ—¶ï¼Œåº”è¯¥ä¸»è¦çœ‹"you"

æ³¨æ„åŠ›æœºåˆ¶è®©æ¨¡å‹èƒ½å¤Ÿ"é€‰æ‹©æ€§åœ°å…³æ³¨"è¾“å…¥çš„ä¸åŒéƒ¨åˆ†ã€‚

## ğŸ“š æ ¸å¿ƒæ¦‚å¿µï¼šQueryã€Keyã€Value

### å›¾ä¹¦é¦†çš„æ¯”å–»

æƒ³è±¡ä½ åœ¨å›¾ä¹¦é¦†æ‰¾ä¹¦ï¼š

1. **Query (æŸ¥è¯¢)**: "æˆ‘æƒ³æ‰¾å…³äºæœºå™¨å­¦ä¹ çš„ä¹¦"
   - è¿™æ˜¯ä½ çš„éœ€æ±‚/é—®é¢˜

2. **Key (é”®)**: æ¯æœ¬ä¹¦çš„æ ‡é¢˜å’Œåˆ†ç±»æ ‡ç­¾
   - "æ·±åº¦å­¦ä¹ å…¥é—¨"ã€"Python ç¼–ç¨‹"ã€"æ•°æ®ç»“æ„"
   - ç”¨æ¥åˆ¤æ–­ä¹¦æ˜¯å¦åŒ¹é…ä½ çš„éœ€æ±‚

3. **Value (å€¼)**: ä¹¦çš„å®é™…å†…å®¹
   - è¿™æ˜¯ä½ æœ€ç»ˆè¦è·å–çš„ä¿¡æ¯

### æ³¨æ„åŠ›çš„å·¥ä½œæµç¨‹

```
1. æ¯”è¾ƒ Query å’Œæ¯ä¸ª Key â†’ å¾—åˆ°ç›¸å…³æ€§åˆ†æ•°
2. ç”¨ Softmax æŠŠåˆ†æ•°è½¬æˆæƒé‡ï¼ˆåŠ èµ·æ¥ç­‰äº 1ï¼‰
3. ç”¨æƒé‡åŠ æƒæ±‚å’Œæ‰€æœ‰ Value
```

## ğŸ”¢ æ‰‹å·¥è®¡ç®—ç¤ºä¾‹

### ç®€å•ä¾‹å­

å‡è®¾æˆ‘ä»¬æœ‰ 3 ä¸ªè¯ï¼Œæ¯ä¸ªè¯ç”¨ 2 ç»´å‘é‡è¡¨ç¤ºï¼š

```python
import numpy as np

# Query: "æˆ‘ç°åœ¨è¦å…³æ³¨ä»€ä¹ˆï¼Ÿ"
Q = np.array([[1.0, 0.0]])  # 1Ã—2

# Keys: "æ¯ä¸ªä½ç½®æœ‰ä»€ä¹ˆä¿¡æ¯ï¼Ÿ"
K = np.array([
    [1.0, 0.0],  # ä½ç½® 0: ä¸»è¦æ˜¯ç»´åº¦ 0
    [0.0, 1.0],  # ä½ç½® 1: ä¸»è¦æ˜¯ç»´åº¦ 1
    [1.0, 0.0]   # ä½ç½® 2: ä¸»è¦æ˜¯ç»´åº¦ 0
])  # 3Ã—2

# Values: "æ¯ä¸ªä½ç½®çš„å®é™…å†…å®¹"
V = np.array([
    [10.0, 0.0],   # ä½ç½® 0 çš„å†…å®¹
    [0.0, 20.0],   # ä½ç½® 1 çš„å†…å®¹
    [30.0, 0.0]    # ä½ç½® 2 çš„å†…å®¹
])  # 3Ã—2
```

### æ­¥éª¤ 1: è®¡ç®—ç›¸ä¼¼åº¦åˆ†æ•°

```python
# QK^T: Query å’Œæ¯ä¸ª Key çš„ç‚¹ç§¯
scores = np.dot(Q, K.T)  # 1Ã—3
print("ç›¸ä¼¼åº¦åˆ†æ•°:", scores)
# è¾“å‡º: [[1.0, 0.0, 1.0]]

# è§£é‡Šï¼š
# - Query å’Œ Key[0] çš„ç›¸ä¼¼åº¦: 1.0 (å¾ˆç›¸ä¼¼!)
# - Query å’Œ Key[1] çš„ç›¸ä¼¼åº¦: 0.0 (ä¸ç›¸ä¼¼)
# - Query å’Œ Key[2] çš„ç›¸ä¼¼åº¦: 1.0 (å¾ˆç›¸ä¼¼!)
```

### æ­¥éª¤ 2: ç¼©æ”¾

```python
d_k = K.shape[-1]  # 2
scaled_scores = scores / np.sqrt(d_k)
print("ç¼©æ”¾å:", scaled_scores)
# è¾“å‡º: [[0.707, 0.0, 0.707]]

# ä¸ºä»€ä¹ˆç¼©æ”¾ï¼Ÿé˜²æ­¢ç‚¹ç§¯å¤ªå¤§ï¼Œå¯¼è‡´ softmax æ¢¯åº¦æ¶ˆå¤±
```

### æ­¥éª¤ 3: Softmax å½’ä¸€åŒ–

```python
from deep_learning.activations import softmax

attention_weights = softmax(scaled_scores, axis=-1)
print("æ³¨æ„åŠ›æƒé‡:", attention_weights)
# è¾“å‡º: [[0.5, 0.0, 0.5]]

# è§£é‡Šï¼š
# - ç»™ä½ç½® 0 åˆ†é… 50% çš„æ³¨æ„åŠ›
# - ç»™ä½ç½® 1 åˆ†é… 0% çš„æ³¨æ„åŠ›
# - ç»™ä½ç½® 2 åˆ†é… 50% çš„æ³¨æ„åŠ›
```

### æ­¥éª¤ 4: åŠ æƒæ±‚å’Œ

```python
output = np.dot(attention_weights, V)  # 1Ã—2
print("è¾“å‡º:", output)
# è¾“å‡º: [[20.0, 0.0]]

# è§£é‡Šï¼š
# output = 0.5 * [10, 0] + 0.0 * [0, 20] + 0.5 * [30, 0]
#        = [5, 0] + [0, 0] + [15, 0]
#        = [20, 0]
```

## ğŸ’¡ å®Œæ•´çš„æ³¨æ„åŠ›å…¬å¼

```
Attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) * V
```

**ç¿»è¯‘**:
1. è®¡ç®— Query å’Œ Key çš„ç›¸ä¼¼åº¦ (QK^T)
2. é™¤ä»¥ sqrt(d_k) è¿›è¡Œç¼©æ”¾
3. ç”¨ Softmax è½¬æˆæƒé‡
4. ç”¨æƒé‡å¯¹ Value åŠ æƒæ±‚å’Œ

## ğŸ§ª å®ç°ä»£ç 

```python
from deep_learning.attention import scaled_dot_product_attention

# åˆ›å»ºè¾“å…¥ (batch_size=1, seq_len=3, d_k=4)
Q = np.random.randn(1, 3, 4)
K = np.random.randn(1, 3, 4)
V = np.random.randn(1, 3, 4)

# è®¡ç®—æ³¨æ„åŠ›
output, attention_weights = scaled_dot_product_attention(Q, K, V)

print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")  # (1, 3, 4)
print(f"æ³¨æ„åŠ›æƒé‡å½¢çŠ¶: {attention_weights.shape}")  # (1, 3, 3)
print(f"æƒé‡æ¯è¡Œçš„å’Œ: {attention_weights[0].sum(axis=1)}")  # [1. 1. 1.]
```

## ğŸ­ å¤šå¤´æ³¨æ„åŠ› (Multi-Head Attention)

### ä¸ºä»€ä¹ˆéœ€è¦å¤šå¤´ï¼Ÿ

å°±åƒä»ä¸åŒè§’åº¦çœ‹åŒä¸€ä¸ªé—®é¢˜ï¼Œå¤šå¤´æ³¨æ„åŠ›è®©æ¨¡å‹èƒ½å¤Ÿï¼š
- å…³æ³¨ä¸åŒç±»å‹çš„æ¨¡å¼
- æ•æ‰å¤šç§å…³ç³»
- å¢å¼ºè¡¨è¾¾èƒ½åŠ›

### æ¯”å–»

å‡è®¾ä½ åœ¨è¯„ä»·ä¸€éƒ¨ç”µå½±ï¼š
- **å¤´ 1**: å…³æ³¨å‰§æƒ…
- **å¤´ 2**: å…³æ³¨æ¼”æŠ€
- **å¤´ 3**: å…³æ³¨ç‰¹æ•ˆ
- **å¤´ 4**: å…³æ³¨é…ä¹

æ¯ä¸ª"å¤´"ä»ä¸åŒè§’åº¦åˆ†æï¼Œæœ€åç»¼åˆæ‰€æœ‰æ„è§ã€‚

### å·¥ä½œåŸç†

```
è¾“å…¥ (d_model=512, num_heads=8)
    â†“
åˆ†æˆ 8 ä¸ªå¤´ (æ¯ä¸ªå¤´ d_k=64)
    â†“
æ¯ä¸ªå¤´ç‹¬ç«‹è®¡ç®—æ³¨æ„åŠ›
    â†“
æ‹¼æ¥æ‰€æœ‰å¤´çš„è¾“å‡º
    â†“
é€šè¿‡çº¿æ€§å±‚æŠ•å½±
    â†“
è¾“å‡º (d_model=512)
```

### ä»£ç ç¤ºä¾‹

```python
from deep_learning.attention import MultiHeadAttention

# åˆ›å»ºå¤šå¤´æ³¨æ„åŠ›
mha = MultiHeadAttention(d_model=128, num_heads=8)

# è¾“å…¥
x = np.random.randn(2, 10, 128)  # (batch, seq_len, d_model)

# è‡ªæ³¨æ„åŠ›
output = mha.forward(x, x, x)

print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")  # å½¢çŠ¶ä¸å˜
print(f"æ¯ä¸ªå¤´çš„ç»´åº¦: {mha.d_k}")  # 128/8 = 16
```

## ğŸ® ç‰¹æ®Šçš„æ³¨æ„åŠ›ï¼šæ©ç 

### 1. å› æœæ©ç  (Causal Mask)

ç”¨äº GPT ç­‰è‡ªå›å½’æ¨¡å‹ï¼Œç¡®ä¿åªèƒ½çœ‹åˆ°ä¹‹å‰çš„è¯ï¼š

```python
from deep_learning.attention import create_causal_mask

mask = create_causal_mask(4)
print(mask)
```

è¾“å‡º:
```
[[  0. -inf -inf -inf]
 [  0.   0. -inf -inf]
 [  0.   0.   0. -inf]
 [  0.   0.   0.   0.]]
```

**è§£é‡Š**:
- ä½ç½® 0 åªèƒ½çœ‹è‡ªå·±
- ä½ç½® 1 èƒ½çœ‹ä½ç½® 0 å’Œ 1
- ä½ç½® 2 èƒ½çœ‹ä½ç½® 0ã€1ã€2
- ä½ç½® 3 èƒ½çœ‹æ‰€æœ‰ä½ç½®

### 2. å¡«å……æ©ç  (Padding Mask)

å¿½ç•¥å¡«å……çš„ä½ç½®ï¼š

```python
from deep_learning.attention import create_padding_mask

# åºåˆ—ï¼Œ0 æ˜¯å¡«å……ç¬¦å·
seq = np.array([[1, 2, 3, 0, 0],
                [4, 5, 0, 0, 0]])

mask = create_padding_mask(seq, pad_token=0)
print(mask.shape)  # (2, 1, 5)
```

## ğŸ§ª å®è·µç»ƒä¹ 

### ç»ƒä¹  1: ç†è§£æ³¨æ„åŠ›æƒé‡

ç»™å®šç®€åŒ–çš„è®¾ç½®ï¼š

```python
Q = np.array([[1.0]])
K = np.array([[1.0], [2.0], [0.5]])
V = np.array([[10.0], [20.0], [5.0]])
```

æ‰‹å·¥è®¡ç®—æ³¨æ„åŠ›è¾“å‡ºã€‚

<details>
<summary>ç‚¹å‡»æŸ¥çœ‹ç­”æ¡ˆ</summary>

```python
# 1. è®¡ç®—åˆ†æ•°
scores = np.dot(Q, K.T)  # [1.0, 2.0, 0.5]

# 2. ç¼©æ”¾
d_k = 1
scaled = scores / np.sqrt(d_k)  # [1.0, 2.0, 0.5]

# 3. Softmax
# exp(1.0) = 2.718, exp(2.0) = 7.389, exp(0.5) = 1.649
# sum = 11.756
weights = [2.718/11.756, 7.389/11.756, 1.649/11.756]
        = [0.231, 0.628, 0.140]

# 4. åŠ æƒæ±‚å’Œ
output = 0.231*10 + 0.628*20 + 0.140*5
       = 2.31 + 12.56 + 0.70
       = 15.57
```
</details>

### ç»ƒä¹  2: å®ç°ç®€å•æ³¨æ„åŠ›

```python
def simple_attention(Q, K, V):
    # ä½ çš„å®ç°
    pass

# æµ‹è¯•
Q = np.array([[1.0, 0.0]])
K = np.array([[1.0, 0.0], [0.0, 1.0]])
V = np.array([[5.0, 0.0], [0.0, 10.0]])

output = simple_attention(Q, K, V)
print(output)
```

<details>
<summary>ç‚¹å‡»æŸ¥çœ‹ç­”æ¡ˆ</summary>

```python
def simple_attention(Q, K, V):
    # 1. è®¡ç®—åˆ†æ•°
    scores = np.dot(Q, K.T)
    
    # 2. ç¼©æ”¾
    d_k = K.shape[-1]
    scaled_scores = scores / np.sqrt(d_k)
    
    # 3. Softmax
    from deep_learning.activations import softmax
    weights = softmax(scaled_scores, axis=-1)
    
    # 4. åŠ æƒæ±‚å’Œ
    output = np.dot(weights, V)
    
    return output
```
</details>

## ğŸ” è¿è¡Œæ¼”ç¤º

```bash
python examples/attention_demo.py
```

è¿™ä¸ªæ¼”ç¤ºä¼šå±•ç¤ºï¼š
1. è‡ªæ³¨æ„åŠ›è¡Œä¸º
2. Query-Key-Value æœºåˆ¶
3. å› æœæ©ç æ•ˆæœ
4. å¤šå¤´æ³¨æ„åŠ›å¯¹æ¯”
5. æ³¨æ„åŠ›æƒé‡å¯è§†åŒ–

## ğŸ“Š å¯è§†åŒ–ç†è§£

æ³¨æ„åŠ›æƒé‡å¯ä»¥ç”»æˆçƒ­åŠ›å›¾ï¼š

```python
import matplotlib.pyplot as plt

# å‡è®¾æœ‰ 5 ä¸ªè¯çš„è‡ªæ³¨æ„åŠ›
seq_len = 5
attention_weights = np.random.rand(seq_len, seq_len)
attention_weights = attention_weights / attention_weights.sum(axis=1, keepdims=True)

plt.figure(figsize=(8, 6))
plt.imshow(attention_weights, cmap='viridis')
plt.colorbar()
plt.xlabel('Key Position')
plt.ylabel('Query Position')
plt.title('Attention Weights Heatmap')
plt.show()
```

## ğŸ“ æ€»ç»“

### å…³é”®è¦ç‚¹

1. **æ³¨æ„åŠ›çš„æœ¬è´¨**: é€‰æ‹©æ€§åœ°å…³æ³¨è¾“å…¥çš„ä¸åŒéƒ¨åˆ†
2. **ä¸‰ä¸ªç»„ä»¶**: Queryï¼ˆæŸ¥è¯¢ï¼‰ã€Keyï¼ˆé”®ï¼‰ã€Valueï¼ˆå€¼ï¼‰
3. **è®¡ç®—æµç¨‹**: ç›¸ä¼¼åº¦ â†’ ç¼©æ”¾ â†’ Softmax â†’ åŠ æƒæ±‚å’Œ
4. **å¤šå¤´ä¼˜åŠ¿**: ä»å¤šä¸ªè§’åº¦æ•æ‰ä¸åŒæ¨¡å¼
5. **æ©ç æœºåˆ¶**: æ§åˆ¶å“ªäº›ä½ç½®å¯ä»¥è¢«å…³æ³¨

### ä¸‹ä¸€æ­¥

- å­¦ä¹  Transformer æ¶æ„
- ç†è§£è‡ªæ³¨æ„åŠ›å’Œäº¤å‰æ³¨æ„åŠ›çš„åŒºåˆ«
- æ¢ç´¢æ³¨æ„åŠ›åœ¨ä¸åŒä»»åŠ¡ä¸­çš„åº”ç”¨

## ğŸ¤” æ€è€ƒé¢˜

1. ä¸ºä»€ä¹ˆè¦é™¤ä»¥ sqrt(d_k) è¿›è¡Œç¼©æ”¾ï¼Ÿ
2. è‡ªæ³¨æ„åŠ›å’Œäº¤å‰æ³¨æ„åŠ›æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ
3. å¤šå¤´æ³¨æ„åŠ›ä¸­ï¼Œæ¯ä¸ªå¤´å­¦åˆ°çš„æ˜¯ä»€ä¹ˆï¼Ÿ

---

**æ­å–œï¼** ä½ å·²ç»ç†è§£äº†æ³¨æ„åŠ›æœºåˆ¶ã€‚è¿™æ˜¯ç†è§£ Transformer çš„å…³é”®ä¸€æ­¥ï¼ğŸ‰
