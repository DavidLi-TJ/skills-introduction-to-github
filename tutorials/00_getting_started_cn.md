# ä»é›¶å¼€å§‹ï¼šå®Œå…¨æ–°æ‰‹æŒ‡å—

## ğŸ‘‹ æ¬¢è¿ï¼

å¦‚æœä½ æ˜¯å®Œå…¨çš„æ–°æ‰‹ï¼Œè¿™ä»½æŒ‡å—ä¼šå¸®åŠ©ä½ ä»é›¶å¼€å§‹å­¦ä¹ æ·±åº¦å­¦ä¹ ã€‚ä¸è¦æ‹…å¿ƒï¼Œæˆ‘ä»¬ä¼šä¸€æ­¥ä¸€æ­¥æ¥ï¼

## ğŸ¯ å­¦ä¹ å‰çš„å‡†å¤‡

### ä½ éœ€è¦çŸ¥é“çš„

**å¿…é¡»**:
- âœ… åŸºç¡€ Pythonï¼ˆå˜é‡ã€å‡½æ•°ã€åˆ—è¡¨ï¼‰
- âœ… èƒ½å¤Ÿè¿è¡Œ Python ç¨‹åº

**ä¸éœ€è¦**:
- âŒ ä¸éœ€è¦æ·±åº¦å­¦ä¹ ç»éªŒ
- âŒ ä¸éœ€è¦é«˜ç­‰æ•°å­¦
- âŒ ä¸éœ€è¦ GPU

### å®‰è£…ç¯å¢ƒ

#### ç¬¬ä¸€æ­¥ï¼šå®‰è£… Python

ç¡®ä¿ä½ æœ‰ Python 3.7 æˆ–æ›´é«˜ç‰ˆæœ¬ï¼š

```bash
python --version
```

å¦‚æœæ²¡æœ‰ï¼Œå» [python.org](https://python.org) ä¸‹è½½å®‰è£…ã€‚

#### ç¬¬äºŒæ­¥ï¼šå…‹éš†ä»“åº“

```bash
git clone https://github.com/DavidLi-TJ/skills-introduction-to-github.git
cd skills-introduction-to-github
```

#### ç¬¬ä¸‰æ­¥ï¼šå®‰è£…ä¾èµ–

```bash
pip install -r requirements.txt
```

å¦‚æœé‡åˆ°é—®é¢˜ï¼Œè¯•è¯•åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼š

```bash
# Windows
python -m venv venv
venv\Scripts\activate

# Mac/Linux
python -m venv venv
source venv/bin/activate

# ç„¶åå®‰è£…
pip install -r requirements.txt
```

#### ç¬¬å››æ­¥ï¼šæµ‹è¯•å®‰è£…

```bash
python examples/softmax_demo.py
```

å¦‚æœçœ‹åˆ°è¾“å‡ºï¼Œè¯´æ˜å®‰è£…æˆåŠŸï¼ğŸ‰

## ğŸ“š å­¦ä¹ è·¯çº¿å›¾

### ç¬¬ 1 å‘¨ï¼šåŸºç¡€æ¦‚å¿µ

#### ç¬¬ 1 å¤©ï¼šä»€ä¹ˆæ˜¯ç¥ç»ç½‘ç»œï¼Ÿ

ç¥ç»ç½‘ç»œå°±åƒä¸€ä¸ª"å‡½æ•°"ï¼š
- **è¾“å…¥**: å›¾ç‰‡ã€æ–‡å­—ã€æ•°å­—...
- **è¾“å‡º**: åˆ†ç±»ã€é¢„æµ‹ã€ç¿»è¯‘...

**ä¾‹å­**: åˆ¤æ–­å›¾ç‰‡æ˜¯çŒ«è¿˜æ˜¯ç‹—
```
å›¾ç‰‡ â†’ [ç¥ç»ç½‘ç»œ] â†’ "è¿™æ˜¯çŒ«ï¼"
```

#### ç¬¬ 2 å¤©ï¼šç†è§£æ¿€æ´»å‡½æ•°

æ¿€æ´»å‡½æ•°æ˜¯ç¥ç»ç½‘ç»œçš„"å¼€å…³"ã€‚

**æœ€ç®€å•çš„ä¾‹å­ - ReLU**:
```python
def relu(x):
    if x > 0:
        return x
    else:
        return 0

print(relu(5))   # è¾“å‡º: 5
print(relu(-3))  # è¾“å‡º: 0
```

**è¿è¡Œä»£ç **:
```python
from deep_learning.activations import relu
import numpy as np

x = np.array([-2, -1, 0, 1, 2])
print(relu(x))
# è¾“å‡º: [0 0 0 1 2]
```

#### ç¬¬ 3-4 å¤©ï¼šå­¦ä¹  Softmax

Softmax æŠŠæ•°å­—å˜æˆæ¦‚ç‡ã€‚

**é˜…è¯»**: [Softmax æ•™ç¨‹](01_softmax_tutorial_cn.md)

**è¿è¡Œ**:
```bash
python examples/softmax_demo.py
```

**åŠ¨æ‰‹ç»ƒä¹ **:
```python
from deep_learning.activations import softmax
import numpy as np

# ä¸‰ä¸ªç±»åˆ«çš„åˆ†æ•°
scores = np.array([3.0, 1.0, 0.2])

# è½¬æˆæ¦‚ç‡
probs = softmax(scores)
print(f"çŒ«: {probs[0]:.1%}")
print(f"ç‹—: {probs[1]:.1%}")
print(f"é¸Ÿ: {probs[2]:.1%}")

# è¾“å‡º:
# çŒ«: 73.1%
# ç‹—: 19.9%
# é¸Ÿ: 7.0%
```

#### ç¬¬ 5-7 å¤©ï¼šç†è§£ç¥ç»ç½‘ç»œå±‚

**Linear å±‚æ˜¯ä»€ä¹ˆï¼Ÿ**

å°±æ˜¯çŸ©é˜µä¹˜æ³•ï¼

```python
from deep_learning.layers import Linear
import numpy as np

# åˆ›å»ºä¸€ä¸ª 3 â†’ 2 çš„å±‚
layer = Linear(in_features=3, out_features=2)

# è¾“å…¥: ä¸€ä¸ª 3 ç»´å‘é‡
x = np.array([[1.0, 2.0, 3.0]])

# è¾“å‡º: å˜æˆ 2 ç»´
output = layer.forward(x)
print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")      # (1, 3)
print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")  # (1, 2)
```

### ç¬¬ 2 å‘¨ï¼šæ³¨æ„åŠ›æœºåˆ¶

#### ç¬¬ 8-10 å¤©ï¼šç†è§£æ³¨æ„åŠ›

**é˜…è¯»**: [æ³¨æ„åŠ›æœºåˆ¶æ•™ç¨‹](02_attention_tutorial_cn.md)

**è¿è¡Œ**:
```bash
python examples/attention_demo.py
```

**æ ¸å¿ƒæ¦‚å¿µ**:
- Query: æˆ‘è¦æ‰¾ä»€ä¹ˆï¼Ÿ
- Key: è¿™é‡Œæœ‰ä»€ä¹ˆï¼Ÿ
- Value: å®é™…å†…å®¹

#### ç¬¬ 11-14 å¤©ï¼šå¤šå¤´æ³¨æ„åŠ›

å¤šå¤´æ³¨æ„åŠ›å°±åƒä»å¤šä¸ªè§’åº¦çœ‹é—®é¢˜ã€‚

```python
from deep_learning.attention import MultiHeadAttention
import numpy as np

# 8 ä¸ªå¤´ï¼Œæ¯ä¸ªå¤´çœ‹ä¸åŒçš„æ–¹é¢
mha = MultiHeadAttention(d_model=64, num_heads=8)

# è¾“å…¥
x = np.random.randn(1, 5, 64)  # 5 ä¸ªè¯ï¼Œæ¯ä¸ª 64 ç»´

# è®¡ç®—æ³¨æ„åŠ›
output = mha.forward(x, x, x)
print(output.shape)  # (1, 5, 64)
```

### ç¬¬ 3 å‘¨ï¼šTransformer

#### ç¬¬ 15-17 å¤©ï¼šTransformer åŸºç¡€

**è¿è¡Œ**:
```bash
python examples/transformer_demo.py
```

**Transformer çš„æ ¸å¿ƒ**:
1. ä½ç½®ç¼–ç ï¼ˆå‘Šè¯‰æ¨¡å‹è¯çš„ä½ç½®ï¼‰
2. å¤šå¤´æ³¨æ„åŠ›ï¼ˆç†è§£è¯ä¹‹é—´çš„å…³ç³»ï¼‰
3. å‰é¦ˆç½‘ç»œï¼ˆè¿›ä¸€æ­¥å¤„ç†ï¼‰

#### ç¬¬ 18-21 å¤©ï¼šå®è·µ Transformer

```python
from deep_learning.transformer import Transformer
import numpy as np

# åˆ›å»ºä¸€ä¸ªå° Transformer
model = Transformer(
    src_vocab_size=1000,   # æºè¯æ±‡é‡
    tgt_vocab_size=1000,   # ç›®æ ‡è¯æ±‡é‡
    d_model=64,            # å°ä¸€ç‚¹çš„ç»´åº¦
    num_heads=4,           # 4 ä¸ªå¤´
    num_encoder_layers=2,  # 2 å±‚ç¼–ç å™¨
    num_decoder_layers=2   # 2 å±‚è§£ç å™¨
)

# æ¨¡æ‹Ÿè¾“å…¥
src = np.array([[1, 2, 3, 4, 5]])  # æºå¥å­
tgt = np.array([[1, 2, 3]])        # ç›®æ ‡å¥å­

# å‰å‘ä¼ æ’­
output = model.forward(src, tgt)
print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
```

### ç¬¬ 4 å‘¨ï¼šç»¼åˆå®è·µ

#### ç¬¬ 22-28 å¤©ï¼šé¡¹ç›®å®è·µ

é€‰æ‹©ä¸€ä¸ªå°é¡¹ç›®ï¼š

1. **æ–‡æœ¬åˆ†ç±»**
   - åˆ¤æ–­è¯„è®ºæ˜¯æ­£é¢è¿˜æ˜¯è´Ÿé¢
   
2. **ç®€å•ç¿»è¯‘**
   - æ•°å­—åºåˆ—è½¬æ¢
   
3. **åºåˆ—é¢„æµ‹**
   - é¢„æµ‹ä¸‹ä¸€ä¸ªæ•°å­—

## ğŸ“ å­¦ä¹ æŠ€å·§

### 1. æ¯å¤©åšæŒ

æ¯å¤© 30 åˆ†é’Ÿæ¯”ä¸€å‘¨å­¦ä¸€æ¬¡æ•ˆæœå¥½ï¼

### 2. åŠ¨æ‰‹å®è·µ

çœ‹æ‡‚ä»£ç ä¸ç­‰äºä¼šå†™ä»£ç ã€‚ä¸€å®šè¦è‡ªå·±æ•²ï¼

### 3. æ”¹ä»£ç ç©

```python
# è¯•ç€æ”¹è¿™äº›å‚æ•°
probs = softmax([1, 2, 3])  # æ”¹æˆ [1, 1, 1] ä¼šæ€æ ·ï¼Ÿ
layer = Linear(5, 3)         # æ”¹æˆ Linear(10, 2) å‘¢ï¼Ÿ
```

### 4. ç”»å›¾ç†è§£

```python
import matplotlib.pyplot as plt

x = np.linspace(-5, 5, 100)
y = relu(x)

plt.plot(x, y)
plt.title('ReLU Function')
plt.show()
```

### 5. é—®é—®é¢˜

ä¸æ‡‚å°±é—®ï¼å¯ä»¥ï¼š
- åœ¨ [Issues](https://github.com/DavidLi-TJ/skills-introduction-to-github/issues) æé—®
- åœ¨ [Discussions](https://github.com/DavidLi-TJ/skills-introduction-to-github/discussions) è®¨è®º

## ğŸ“– å¿…å¤‡èµ„æº

### Python åŸºç¡€

å¦‚æœ Python ä¸ç†Ÿç»ƒï¼š
- [å»–é›ªå³° Python æ•™ç¨‹](https://www.liaoxuefeng.com/wiki/1016959663602400)
- [èœé¸Ÿæ•™ç¨‹](https://www.runoob.com/python3/python3-tutorial.html)

### NumPy åŸºç¡€

NumPy æ˜¯å¿…é¡»çš„ï¼š
```python
import numpy as np

# åˆ›å»ºæ•°ç»„
a = np.array([1, 2, 3])

# æ•°ç»„è¿ç®—
b = a * 2  # [2, 4, 6]

# çŸ©é˜µä¹˜æ³•
A = np.array([[1, 2], [3, 4]])
B = np.array([[5, 6], [7, 8]])
C = np.dot(A, B)
```

### çº¿æ€§ä»£æ•°

éœ€è¦äº†è§£çš„æ¦‚å¿µï¼š
- å‘é‡å’ŒçŸ©é˜µ
- çŸ©é˜µä¹˜æ³•
- ç‚¹ç§¯

**ä¸éœ€è¦**: å¤æ‚çš„è¯æ˜å’Œæ¨å¯¼

## ğŸ› å¸¸è§é—®é¢˜

### Q: æ•°å­¦ä¸å¥½æ€ä¹ˆåŠï¼Ÿ

**A**: ä¸ç”¨æ‹…å¿ƒï¼
- æˆ‘ä»¬ä»ç›´è§‰å¼€å§‹
- ä»£ç æ¯”å…¬å¼æ›´é‡è¦
- è¾¹å­¦è¾¹è¡¥æ•°å­¦

### Q: ä»£ç æŠ¥é”™æ€ä¹ˆåŠï¼Ÿ

**A**: æŒ‰è¿™ä¸ªé¡ºåºæ£€æŸ¥ï¼š

1. **çœ‹é”™è¯¯ä¿¡æ¯**
```python
# å¦‚æœæ˜¯ ImportError
pip install -r requirements.txt

# å¦‚æœæ˜¯ ModuleNotFoundError
# ç¡®ä¿åœ¨æ­£ç¡®çš„ç›®å½•
cd /path/to/skills-introduction-to-github
```

2. **æ£€æŸ¥ Python ç‰ˆæœ¬**
```bash
python --version  # åº”è¯¥æ˜¯ 3.7+
```

3. **é‡æ–°å®‰è£…**
```bash
pip uninstall numpy matplotlib pytest
pip install -r requirements.txt
```

### Q: æˆ‘åº”è¯¥è®°ä½æ‰€æœ‰å…¬å¼å—ï¼Ÿ

**A**: ä¸éœ€è¦ï¼
- ç†è§£æ¦‚å¿µæ¯”è®°å…¬å¼é‡è¦
- ä»£ç å°±æ˜¯å…¬å¼çš„å®ç°
- ç”¨çš„æ—¶å€™æŸ¥å°±è¡Œ

### Q: å­¦ä¹ éœ€è¦å¤šé•¿æ—¶é—´ï¼Ÿ

**A**: å› äººè€Œå¼‚ï¼Œä½†å¤§æ¦‚ï¼š
- 1 ä¸ªæœˆï¼šæŒæ¡åŸºç¡€
- 3 ä¸ªæœˆï¼šèƒ½å®ç°ç®€å•æ¨¡å‹
- 6 ä¸ªæœˆï¼šèƒ½çœ‹æ‡‚è®ºæ–‡

## ğŸ¯ å­¦ä¹ æ£€æŸ¥æ¸…å•

### åŸºç¡€çº§ï¼ˆç¬¬ 1-2 å‘¨ï¼‰

- [ ] èƒ½è¿è¡Œæ‰€æœ‰ç¤ºä¾‹ä»£ç 
- [ ] ç†è§£ Softmax çš„ä½œç”¨
- [ ] èƒ½è§£é‡Š ReLU ä¸ºä»€ä¹ˆæœ‰ç”¨
- [ ] çŸ¥é“ä»€ä¹ˆæ˜¯ Linear å±‚
- [ ] ç†è§£çŸ©é˜µä¹˜æ³•çš„å½¢çŠ¶å˜åŒ–

### è¿›é˜¶çº§ï¼ˆç¬¬ 3-4 å‘¨ï¼‰

- [ ] ç†è§£æ³¨æ„åŠ›æœºåˆ¶çš„ç›´è§‰
- [ ] èƒ½è§£é‡Š Queryã€Keyã€Value
- [ ] çŸ¥é“ä¸ºä»€ä¹ˆéœ€è¦å¤šå¤´æ³¨æ„åŠ›
- [ ] ç†è§£ Transformer çš„ç»“æ„
- [ ] èƒ½ä¿®æ”¹ä»£ç å‚æ•°å¹¶é¢„æµ‹ç»“æœ

### é«˜çº§ï¼ˆä¹‹åï¼‰

- [ ] èƒ½ä»é›¶å®ç°ç®€å•çš„æ³¨æ„åŠ›
- [ ] èƒ½è§£é‡Šä½ç½®ç¼–ç çš„ä½œç”¨
- [ ] ç†è§£æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–
- [ ] èƒ½åœ¨å°æ•°æ®é›†ä¸Šè®­ç»ƒæ¨¡å‹

## ğŸ’ª ä»Šå¤©å°±å¼€å§‹ï¼

ä¸è¦æƒ³å¤ªå¤šï¼Œä»æœ€ç®€å•çš„å¼€å§‹ï¼š

```bash
# 1. è¿è¡Œç¬¬ä¸€ä¸ªç¤ºä¾‹
python examples/softmax_demo.py

# 2. æ‰“å¼€ä»£ç çœ‹çœ‹
# ç”¨ä»»ä½•æ–‡æœ¬ç¼–è¾‘å™¨æ‰“å¼€ deep_learning/activations.py

# 3. è¯•ç€ä¿®æ”¹
# æ”¹å˜ softmax_demo.py é‡Œçš„è¾“å…¥å€¼ï¼Œçœ‹è¾“å‡ºå˜åŒ–
```

è®°ä½ï¼š
- ğŸ¢ æ…¢æ…¢æ¥ï¼Œä¸è¦æ€¥
- ğŸ’ª æ¯å¤©è¿›æ­¥ä¸€ç‚¹ç‚¹
- ğŸ¤ é‡åˆ°é—®é¢˜å°±é—®
- ğŸ‰ äº«å—å­¦ä¹ çš„è¿‡ç¨‹

**ç¥ä½ å­¦ä¹ æ„‰å¿«ï¼ä½ ä¸€å®šå¯ä»¥çš„ï¼ğŸš€**

---

## ğŸ“ éœ€è¦å¸®åŠ©ï¼Ÿ

- ğŸ“§ [æäº¤ Issue](https://github.com/DavidLi-TJ/skills-introduction-to-github/issues)
- ğŸ’¬ [å‚ä¸è®¨è®º](https://github.com/DavidLi-TJ/skills-introduction-to-github/discussions)
- ğŸ“š æŸ¥çœ‹å…¶ä»–æ•™ç¨‹ï¼š
  - [Softmax è¯¦è§£](01_softmax_tutorial_cn.md)
  - [æ³¨æ„åŠ›æœºåˆ¶è¯¦è§£](02_attention_tutorial_cn.md)
